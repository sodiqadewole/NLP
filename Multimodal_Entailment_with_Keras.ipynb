{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Multimodal entailment involves analysis of text, images, audio, and video data sources to determine if a piece of information contradict another or whether a given piece of information implies the other. This is applied is social media content moderation where platform operators audits and moderate content."
      ],
      "metadata": {
        "id": "uuCqdUs4Vsx3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vswQBJYZVl5Y"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "os.environ['KERAS_BACKEND'] = 'jax'\n",
        "\n",
        "import keras\n",
        "import keras_hub\n",
        "from keras.utils import PyDataset"
      ],
      "metadata": {
        "id": "B-gUuHXAWsh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a label map\n",
        "label_map = {\n",
        "    'contradiction': 0,\n",
        "    'entailment': 1,\n",
        "    'neutral': 2\n",
        "}"
      ],
      "metadata": {
        "id": "qVlyTNDoWso1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect dataset\n",
        "image_base_path = keras.utils.get_file(\n",
        "    \"tweet_images\",\n",
        "    \"https://github.com/sayakpaul/Multimodal-Entailment-Baseline/releases/download/v1.0.0/tweet_images.tar.gz\",\n",
        "    untar=True,\n",
        ")\n",
        "\n",
        "# Read dataset and apply preprocessing to the first 1k samples\n",
        "df = pd.read_csv(\n",
        "    \"https://github.com/sayakpaul/Multimodal-Entailment-Baseline/raw/main/csvs/tweets.csv\"\n",
        ").iloc[0:1000]\n",
        "\n",
        "df.sample(10)"
      ],
      "metadata": {
        "id": "Ot9MJD5JWsr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We formulate the entailment task as - given pairs of {text_1, image_1} and {text_2, image_2}, predict (contradict, entail, or neutral)\n",
        "\n",
        "images_1_paths = []\n",
        "images_2_paths = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    current_row = df.iloc[i]\n",
        "    id_1 = current_row[\"id_1\"]\n",
        "    id_2 = current_row[\"id_2\"]\n",
        "    extension_1 = current_row[\"image_1\"].split(\".\")[-1]\n",
        "    extension_2 = current_row[\"image_2\"].split(\".\")[-1]\n",
        "    image_1_path = os.path.join(image_base_path, f\"{id_1}.{extension_1}\")\n",
        "    image_2_path = os.path.join(image_base_path, f\"{id_2}.{extension_2}\")\n",
        "    images_1_paths.append(image_1_path)\n",
        "    images_2_paths.append(image_2_path)\n",
        "\n",
        "df[\"image_1_path\"] = images_1_paths\n",
        "df[\"image_2_path\"] = images_2_paths\n",
        "\n",
        "# Add label column\n",
        "df['label_idx'] = df['label'].apply(lambda x: label_map[x])"
      ],
      "metadata": {
        "id": "e1IV7DzcXpSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize sample dataset\n",
        "def visualize(idx):\n",
        "    current_row = df.iloc[idx]\n",
        "    image_1 = plt.imread(current_row[\"image_1_path\"])\n",
        "    image_2 = plt.imread(current_row[\"image_2_path\"])\n",
        "    text_1 = current_row[\"text_1\"]\n",
        "    text_2 = current_row[\"text_2\"]\n",
        "    label = current_row[\"label\"]\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image_1)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Image 1: {text_1}\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(image_2)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Image 2: {text_2}\")\n",
        "    plt.show()\n",
        "    print(f\"Label: {label}\")\n",
        "\n",
        "random_idx = random.choice(range(len(df)))\n",
        "visualize(random_idx)\n",
        "\n",
        "random_idx = random.choice(range(len(df)))\n",
        "visualize(random_idx)\n"
      ],
      "metadata": {
        "id": "eTB-ELzXXpVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-Test split\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'].values, random_state=42)\n",
        "\n",
        "# validation set\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.05, stratify=train_df['label'].values, random_state=42)\n",
        "\n",
        "print(f\"Total train examples: {len(train_df)}\")\n",
        "print(f\"Total val examples: {len(val_df)}\")\n",
        "print(f\"Total test examples: {len(test_df)}\""
      ],
      "metadata": {
        "id": "t3nO1-uBalKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data input pipeline\n",
        "text_preprocessor = keras_hub.models.BertTextClassifierPreprocessor.from_preset(\"bert_base_en_uncased\", sequence_lengh=128,\n",
        ")"
      ],
      "metadata": {
        "id": "4j0O8FJCalQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the preprocessor on a sample input\n",
        "idx = random.choice(range(len(train_df)))\n",
        "sample_input = train_df.iloc[idx]\n",
        "sample_text_1 = sample_input[\"text_1\"]\n",
        "sample_text_2 = sample_input[\"text_2\"]\n",
        "\n",
        "print(f\"Text 1: {sample_text_1}\")\n",
        "print(f\"Text 2: {sample_text_2}\")\n",
        "\n",
        "processed_text = text_preprocessor([sample_text_1, sample_text_2])\n",
        "print(processed_text)\n",
        "\n",
        "print(\"Keys            : \", list(processed_text.keys()))\n",
        "print(\"Shape Token Ids : \", processed_text['token_ids'].shape)\n",
        "print(\"Token Ids       : \", processed_text['token_ids'][0, :16])\n",
        "print(\"Shape Padding Masks     : \", processed_text['padding_mask'].shape)\n",
        "print(\"Padding Masks     : \", processed_text['padding_mask'][0, :16])\n",
        "print(\"Shape Segment Ids : \", processed_text['segment_ids'].shape)\n",
        "print(\"Segment Ids       : \", processed_text['segment_ids'][0, :16])"
      ],
      "metadata": {
        "id": "IZaq6M0ralTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tf.data.Dataset objects from dataframes\n",
        "def dataframe_to_dataset(dataframe):\n",
        "    columns = ['image_1_path', 'image_2_path', 'text_1', 'text_2', 'label_idx']\n",
        "    dataset = UnifiedPyDataset(dataframe, batch_size=32, workers=4)\n",
        "    return dataset\n",
        "\n",
        "# Preprocessing utiliteis\n",
        "bert_input_features = ['padding_mask', 'segment_ids', 'token_ids']\n",
        "def preprocess_text(text_1, text_2):\n",
        "    output = text_preprocessor([text_1, text_2])\n",
        "    return {feature: keras.ops.reshape(output[feature], [-1]) for feature in bert_input_features}"
      ],
      "metadata": {
        "id": "YY8jf_gFalWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnifiedPyDataset(PyDataset):\n",
        "    \"\"\"A Keras-compatible dataset that processes a DataFrame for TensorFlow, JAX, and PyTorch.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        df,\n",
        "        batch_size=32,\n",
        "        workers=4,\n",
        "        use_multiprocessing=False,\n",
        "        max_queue_size=10,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            df: pandas DataFrame with data\n",
        "            batch_size: Batch size for dataset\n",
        "            workers: Number of workers to use for parallel loading (Keras)\n",
        "            use_multiprocessing: Whether to use multiprocessing\n",
        "            max_queue_size: Maximum size of the data queue for parallel loading\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.dataframe = df\n",
        "        columns = [\"image_1_path\", \"image_2_path\", \"text_1\", \"text_2\"]\n",
        "\n",
        "        # image files\n",
        "        self.image_x_1 = self.dataframe[\"image_1_path\"]\n",
        "        self.image_x_2 = self.dataframe[\"image_1_path\"]\n",
        "        self.image_y = self.dataframe[\"label_idx\"]\n",
        "\n",
        "        # text files\n",
        "        self.text_x_1 = self.dataframe[\"text_1\"]\n",
        "        self.text_x_2 = self.dataframe[\"text_2\"]\n",
        "        self.text_y = self.dataframe[\"label_idx\"]\n",
        "\n",
        "        # general\n",
        "        self.batch_size = batch_size\n",
        "        self.workers = workers\n",
        "        self.use_multiprocessing = use_multiprocessing\n",
        "        self.max_queue_size = max_queue_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Fetches a batch of data from the dataset at the given index.\n",
        "        \"\"\"\n",
        "\n",
        "        # Return x, y for batch idx.\n",
        "        low = index * self.batch_size\n",
        "        # Cap upper bound at array length; the last batch may be smaller\n",
        "        # if the total number of items is not a multiple of batch size.\n",
        "\n",
        "        high_image_1 = min(low + self.batch_size, len(self.image_x_1))\n",
        "        high_image_2 = min(low + self.batch_size, len(self.image_x_2))\n",
        "\n",
        "        high_text_1 = min(low + self.batch_size, len(self.text_x_1))\n",
        "        high_text_2 = min(low + self.batch_size, len(self.text_x_1))\n",
        "\n",
        "        # images files\n",
        "        batch_image_x_1 = self.image_x_1[low:high_image_1]\n",
        "        batch_image_y_1 = self.image_y[low:high_image_1]\n",
        "\n",
        "        batch_image_x_2 = self.image_x_2[low:high_image_2]\n",
        "        batch_image_y_2 = self.image_y[low:high_image_2]\n",
        "\n",
        "        # text files\n",
        "        batch_text_x_1 = self.text_x_1[low:high_text_1]\n",
        "        batch_text_y_1 = self.text_y[low:high_text_1]\n",
        "\n",
        "        batch_text_x_2 = self.text_x_2[low:high_text_2]\n",
        "        batch_text_y_2 = self.text_y[low:high_text_2]\n",
        "\n",
        "        # image number 1 inputs\n",
        "        image_1 = [\n",
        "            resize(imread(file_name), (128, 128)) for file_name in batch_image_x_1\n",
        "        ]\n",
        "        image_1 = [\n",
        "            (  # exeperienced some shapes which were different from others.\n",
        "                np.array(Image.fromarray((img.astype(np.uint8))).convert(\"RGB\"))\n",
        "                if img.shape[2] == 4\n",
        "                else img\n",
        "            )\n",
        "            for img in image_1\n",
        "        ]\n",
        "        image_1 = np.array(image_1)\n",
        "\n",
        "        # Both text inputs to the model, return a dict for inputs to BertBackbone\n",
        "        text = {\n",
        "            key: np.array(\n",
        "                [\n",
        "                    d[key]\n",
        "                    for d in [\n",
        "                        preprocess_text(file_path1, file_path2)\n",
        "                        for file_path1, file_path2 in zip(\n",
        "                            batch_text_x_1, batch_text_x_2\n",
        "                        )\n",
        "                    ]\n",
        "                ]\n",
        "            )\n",
        "            for key in [\"padding_mask\", \"token_ids\", \"segment_ids\"]\n",
        "        }\n",
        "\n",
        "        # Image number 2 model inputs\n",
        "        image_2 = [\n",
        "            resize(imread(file_name), (128, 128)) for file_name in batch_image_x_2\n",
        "        ]\n",
        "        image_2 = [\n",
        "            (  # exeperienced some shapes which were different from others\n",
        "                np.array(Image.fromarray((img.astype(np.uint8))).convert(\"RGB\"))\n",
        "                if img.shape[2] == 4\n",
        "                else img\n",
        "            )\n",
        "            for img in image_2\n",
        "        ]\n",
        "        # Stack the list comprehension to an nd.array\n",
        "        image_2 = np.array(image_2)\n",
        "\n",
        "        return (\n",
        "            {\n",
        "                \"image_1\": image_1,\n",
        "                \"image_2\": image_2,\n",
        "                \"padding_mask\": text[\"padding_mask\"],\n",
        "                \"segment_ids\": text[\"segment_ids\"],\n",
        "                \"token_ids\": text[\"token_ids\"],\n",
        "            },\n",
        "            # Target lables\n",
        "            np.array(batch_image_y_1),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the number of batches in the dataset.\n",
        "        \"\"\"\n",
        "        return math.ceil(len(self.dataframe) / self.batch_size)"
      ],
      "metadata": {
        "id": "IiAd9zm2alZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zP01FOMnalcz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}