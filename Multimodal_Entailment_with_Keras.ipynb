{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Multimodal entailment involves analysis of text, images, audio, and video data sources to determine if a piece of information contradict another or whether a given piece of information implies the other. This is applied is social media content moderation where platform operators audits and moderate content."
      ],
      "metadata": {
        "id": "uuCqdUs4Vsx3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vswQBJYZVl5Y"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "os.environ['KERAS_BACKEND'] = 'jax'\n",
        "\n",
        "import keras\n",
        "import keras_hub\n",
        "from keras.utils import PyDataset"
      ],
      "metadata": {
        "id": "B-gUuHXAWsh6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a label map\n",
        "label_map = {\n",
        "    'Contradictory': 0,\n",
        "    'Implies': 1,\n",
        "    'NoEntailment': 2\n",
        "}"
      ],
      "metadata": {
        "id": "qVlyTNDoWso1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect dataset\n",
        "image_base_path = keras.utils.get_file(\n",
        "    \"tweet_images\",\n",
        "    \"https://github.com/sayakpaul/Multimodal-Entailment-Baseline/releases/download/v1.0.0/tweet_images.tar.gz\",\n",
        "    untar=True,\n",
        ")\n",
        "\n",
        "# Read dataset and apply preprocessing to the first 1k samples\n",
        "df = pd.read_csv(\n",
        "    \"https://github.com/sayakpaul/Multimodal-Entailment-Baseline/raw/main/csvs/tweets.csv\"\n",
        ").iloc[0:1000]\n",
        "\n",
        "df.sample(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "id": "Ot9MJD5JWsr0",
        "outputId": "f44f2d76-858f-4dc8-a6b3-1cc81a014f07"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/sayakpaul/Multimodal-Entailment-Baseline/releases/download/v1.0.0/tweet_images.tar.gz\n",
            "\u001b[1m344273442/344273442\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    id_1                                             text_1  \\\n",
              "927  1334686373266198529  Solar-powered holiday lights are convenient, m...   \n",
              "924  1378332375923499012  I just added Invincible (2021) to my library! ...   \n",
              "247  1353726537741312001  Total #COVID19 cases by age group (change from...   \n",
              "449  1373696320612003843  Heard Island and McDonald Islands still numero...   \n",
              "938  1364042142243504131  #NHL Impact Card for New York Islanders on 202...   \n",
              "485  1342913748680519688  Lineups üá∏üá™ vs üá®üáø\\n#worldjuniors https://t.co/j...   \n",
              "160  1359907916489318405  $SPY Today (8:30 CST), equities higher, FI mix...   \n",
              "469  1347963434550308867  WE'RE HYPED AND READY TO ENJOY #SUPERWILDCARDW...   \n",
              "643  1363989251629740033  Mon 17:00: Sunny; Temp 2 C; Wind W 25 km/h; Hu...   \n",
              "299  1362935097079521283  1st quarter of the Boys Varsity Basketball gam...   \n",
              "\n",
              "                                            image_1                 id_2  \\\n",
              "927  http://pbs.twimg.com/media/EoXCLslXIAMBFt5.jpg  1339307380450988034   \n",
              "924  http://pbs.twimg.com/media/EyDR_bjXEAQDjeb.jpg  1380680917585383425   \n",
              "247  http://pbs.twimg.com/media/EslnEuJW8AESfdu.jpg  1360249815418945536   \n",
              "449  http://pbs.twimg.com/media/ExBZhhlW8AQBw6u.jpg  1374421735206887425   \n",
              "938  http://pbs.twimg.com/media/Eu4NGaTWQAEeQ_f.png  1374174084821614602   \n",
              "485  http://pbs.twimg.com/media/EqLmNwdW8AQ8W-1.jpg  1346232678744526848   \n",
              "160  http://pbs.twimg.com/media/Et88sRHXMAEXg-T.jpg  1360234816134905862   \n",
              "469  http://pbs.twimg.com/media/ErTtmZFWMAM6zir.jpg  1350554189416583172   \n",
              "643  http://pbs.twimg.com/media/Eu3c_uZXAAg82fz.png  1371071703888101381   \n",
              "299  http://pbs.twimg.com/media/EuoePpoXYAEfv-m.png  1364366166282633218   \n",
              "\n",
              "                                                text_2  \\\n",
              "927  Solar-powered holiday lights are convenient, m...   \n",
              "924  I've just watched episode S01 | E05 of Invinci...   \n",
              "247  Total #COVID19 cases by age group (change from...   \n",
              "449  Heard Island and McDonald Islands still leads!...   \n",
              "938  #NHL Impact Card for New York Islanders on 202...   \n",
              "485  Line-ups for the semi-final between Canada and...   \n",
              "160  $SPY Today (8:30 CST), equities mixed, FI lowe...   \n",
              "469  WE'RE HYPED AND READY FOR  #DIVISIONALROUND üèà ...   \n",
              "643  Sun 08:00: Mainly Sunny; Temp -15.8 C; Windchi...   \n",
              "299  1st quarter of the Girls Varsity Basketball ga...   \n",
              "\n",
              "                                            image_2          label  \n",
              "927  http://pbs.twimg.com/media/EpYs9BaWEAAa2gI.jpg   NoEntailment  \n",
              "924  http://pbs.twimg.com/media/Eykp-lFWUAU5PHD.jpg   NoEntailment  \n",
              "247  http://pbs.twimg.com/media/EuCT42TXIAAeWh8.jpg   NoEntailment  \n",
              "449  http://pbs.twimg.com/media/ExLtSJfUcAwW9E8.jpg   NoEntailment  \n",
              "938  http://pbs.twimg.com/media/ExIMDFjXAAIufEh.png   NoEntailment  \n",
              "485  http://pbs.twimg.com/media/Eq62ZlcW4AUZ4uH.jpg   NoEntailment  \n",
              "160  http://pbs.twimg.com/media/EuCGWzfXYAM7Cmg.jpg  Contradictory  \n",
              "469  http://pbs.twimg.com/media/Er4h4JaXUAA2zUN.jpg   NoEntailment  \n",
              "643  http://pbs.twimg.com/media/EwcGctnWgAQqUIA.png   NoEntailment  \n",
              "299  http://pbs.twimg.com/media/Eu8zzEjUcAAjHU1.jpg   NoEntailment  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-74dceeb7-e094-49c0-9d56-59b74ca04d24\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_1</th>\n",
              "      <th>text_1</th>\n",
              "      <th>image_1</th>\n",
              "      <th>id_2</th>\n",
              "      <th>text_2</th>\n",
              "      <th>image_2</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>927</th>\n",
              "      <td>1334686373266198529</td>\n",
              "      <td>Solar-powered holiday lights are convenient, m...</td>\n",
              "      <td>http://pbs.twimg.com/media/EoXCLslXIAMBFt5.jpg</td>\n",
              "      <td>1339307380450988034</td>\n",
              "      <td>Solar-powered holiday lights are convenient, m...</td>\n",
              "      <td>http://pbs.twimg.com/media/EpYs9BaWEAAa2gI.jpg</td>\n",
              "      <td>NoEntailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>924</th>\n",
              "      <td>1378332375923499012</td>\n",
              "      <td>I just added Invincible (2021) to my library! ...</td>\n",
              "      <td>http://pbs.twimg.com/media/EyDR_bjXEAQDjeb.jpg</td>\n",
              "      <td>1380680917585383425</td>\n",
              "      <td>I've just watched episode S01 | E05 of Invinci...</td>\n",
              "      <td>http://pbs.twimg.com/media/Eykp-lFWUAU5PHD.jpg</td>\n",
              "      <td>NoEntailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>1353726537741312001</td>\n",
              "      <td>Total #COVID19 cases by age group (change from...</td>\n",
              "      <td>http://pbs.twimg.com/media/EslnEuJW8AESfdu.jpg</td>\n",
              "      <td>1360249815418945536</td>\n",
              "      <td>Total #COVID19 cases by age group (change from...</td>\n",
              "      <td>http://pbs.twimg.com/media/EuCT42TXIAAeWh8.jpg</td>\n",
              "      <td>NoEntailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>449</th>\n",
              "      <td>1373696320612003843</td>\n",
              "      <td>Heard Island and McDonald Islands still numero...</td>\n",
              "      <td>http://pbs.twimg.com/media/ExBZhhlW8AQBw6u.jpg</td>\n",
              "      <td>1374421735206887425</td>\n",
              "      <td>Heard Island and McDonald Islands still leads!...</td>\n",
              "      <td>http://pbs.twimg.com/media/ExLtSJfUcAwW9E8.jpg</td>\n",
              "      <td>NoEntailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>938</th>\n",
              "      <td>1364042142243504131</td>\n",
              "      <td>#NHL Impact Card for New York Islanders on 202...</td>\n",
              "      <td>http://pbs.twimg.com/media/Eu4NGaTWQAEeQ_f.png</td>\n",
              "      <td>1374174084821614602</td>\n",
              "      <td>#NHL Impact Card for New York Islanders on 202...</td>\n",
              "      <td>http://pbs.twimg.com/media/ExIMDFjXAAIufEh.png</td>\n",
              "      <td>NoEntailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>485</th>\n",
              "      <td>1342913748680519688</td>\n",
              "      <td>Lineups üá∏üá™ vs üá®üáø\\n#worldjuniors https://t.co/j...</td>\n",
              "      <td>http://pbs.twimg.com/media/EqLmNwdW8AQ8W-1.jpg</td>\n",
              "      <td>1346232678744526848</td>\n",
              "      <td>Line-ups for the semi-final between Canada and...</td>\n",
              "      <td>http://pbs.twimg.com/media/Eq62ZlcW4AUZ4uH.jpg</td>\n",
              "      <td>NoEntailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>1359907916489318405</td>\n",
              "      <td>$SPY Today (8:30 CST), equities higher, FI mix...</td>\n",
              "      <td>http://pbs.twimg.com/media/Et88sRHXMAEXg-T.jpg</td>\n",
              "      <td>1360234816134905862</td>\n",
              "      <td>$SPY Today (8:30 CST), equities mixed, FI lowe...</td>\n",
              "      <td>http://pbs.twimg.com/media/EuCGWzfXYAM7Cmg.jpg</td>\n",
              "      <td>Contradictory</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>469</th>\n",
              "      <td>1347963434550308867</td>\n",
              "      <td>WE'RE HYPED AND READY TO ENJOY #SUPERWILDCARDW...</td>\n",
              "      <td>http://pbs.twimg.com/media/ErTtmZFWMAM6zir.jpg</td>\n",
              "      <td>1350554189416583172</td>\n",
              "      <td>WE'RE HYPED AND READY FOR  #DIVISIONALROUND üèà ...</td>\n",
              "      <td>http://pbs.twimg.com/media/Er4h4JaXUAA2zUN.jpg</td>\n",
              "      <td>NoEntailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>643</th>\n",
              "      <td>1363989251629740033</td>\n",
              "      <td>Mon 17:00: Sunny; Temp 2 C; Wind W 25 km/h; Hu...</td>\n",
              "      <td>http://pbs.twimg.com/media/Eu3c_uZXAAg82fz.png</td>\n",
              "      <td>1371071703888101381</td>\n",
              "      <td>Sun 08:00: Mainly Sunny; Temp -15.8 C; Windchi...</td>\n",
              "      <td>http://pbs.twimg.com/media/EwcGctnWgAQqUIA.png</td>\n",
              "      <td>NoEntailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>1362935097079521283</td>\n",
              "      <td>1st quarter of the Boys Varsity Basketball gam...</td>\n",
              "      <td>http://pbs.twimg.com/media/EuoePpoXYAEfv-m.png</td>\n",
              "      <td>1364366166282633218</td>\n",
              "      <td>1st quarter of the Girls Varsity Basketball ga...</td>\n",
              "      <td>http://pbs.twimg.com/media/Eu8zzEjUcAAjHU1.jpg</td>\n",
              "      <td>NoEntailment</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-74dceeb7-e094-49c0-9d56-59b74ca04d24')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-74dceeb7-e094-49c0-9d56-59b74ca04d24 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-74dceeb7-e094-49c0-9d56-59b74ca04d24');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-37f0125f-9b54-45b5-9aad-44a5376f08dc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-37f0125f-9b54-45b5-9aad-44a5376f08dc')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-37f0125f-9b54-45b5-9aad-44a5376f08dc button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"id_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13539195044829276,\n        \"min\": 1334686373266198529,\n        \"max\": 1378332375923499012,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          1363989251629740033,\n          1378332375923499012,\n          1342913748680519688\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text_1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Mon 17:00: Sunny; Temp 2 C; Wind W 25 km/h; Humidity 72%; Press 100.4 kPa / rising. https://t.co/5KpmSf5xAR\",\n          \"I just added Invincible (2021) to my library! #tvtime https://t.co/JYMjQacHdk https://t.co/sXdOeDuLxc\",\n          \"Lineups \\ud83c\\uddf8\\ud83c\\uddea vs \\ud83c\\udde8\\ud83c\\uddff\\n#worldjuniors https://t.co/jzsD8YzM5J\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image_1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"http://pbs.twimg.com/media/Eu3c_uZXAAg82fz.png\",\n          \"http://pbs.twimg.com/media/EyDR_bjXEAQDjeb.jpg\",\n          \"http://pbs.twimg.com/media/EqLmNwdW8AQ8W-1.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13498809714830086,\n        \"min\": 1339307380450988034,\n        \"max\": 1380680917585383425,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          1371071703888101381,\n          1380680917585383425,\n          1346232678744526848\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text_2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Sun 08:00: Mainly Sunny; Temp -15.8 C; Windchill -28; Wind NW 35 km/h; Humidity 62%; Press 103.1 kPa / rising. https://t.co/Kt5XKRNJZW\",\n          \"I've just watched episode S01 | E05 of Invincible (2021)! #invincible  https://t.co/oJSFqhjMMv #tvtime https://t.co/wiKQmXGiJd\",\n          \"Line-ups for the semi-final between Canada and Russia! #WorldJuniors @FullPressNHL https://t.co/alOEPPZIzY\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image_2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"http://pbs.twimg.com/media/EwcGctnWgAQqUIA.png\",\n          \"http://pbs.twimg.com/media/Eykp-lFWUAU5PHD.jpg\",\n          \"http://pbs.twimg.com/media/Eq62ZlcW4AUZ4uH.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Contradictory\",\n          \"NoEntailment\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7NBkMykHucVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We formulate the entailment task as - given pairs of {text_1, image_1} and {text_2, image_2}, predict (contradict, entail, or neutral)\n",
        "\n",
        "images_1_paths = []\n",
        "images_2_paths = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    current_row = df.iloc[i]\n",
        "    id_1 = current_row[\"id_1\"]\n",
        "    id_2 = current_row[\"id_2\"]\n",
        "    extension_1 = current_row[\"image_1\"].split(\".\")[-1]\n",
        "    extension_2 = current_row[\"image_2\"].split(\".\")[-1]\n",
        "    image_1_path = os.path.join(image_base_path, f\"{id_1}.{extension_1}\")\n",
        "    image_2_path = os.path.join(image_base_path, f\"{id_2}.{extension_2}\")\n",
        "    images_1_paths.append(image_1_path)\n",
        "    images_2_paths.append(image_2_path)\n",
        "\n",
        "df[\"image_1_path\"] = images_1_paths\n",
        "df[\"image_2_path\"] = images_2_paths\n",
        "\n",
        "# Add label column\n",
        "df['label_idx'] = df['label'].apply(lambda x: label_map[x])"
      ],
      "metadata": {
        "id": "e1IV7DzcXpSS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize sample dataset\n",
        "def visualize(idx):\n",
        "    current_row = df.iloc[idx]\n",
        "    image_1 = plt.imread(current_row[\"image_1_path\"])\n",
        "    image_2 = plt.imread(current_row[\"image_2_path\"])\n",
        "    text_1 = current_row[\"text_1\"]\n",
        "    text_2 = current_row[\"text_2\"]\n",
        "    label = current_row[\"label\"]\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image_1)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Image 1: {text_1}\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(image_2)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Image 2: {text_2}\")\n",
        "    plt.show()\n",
        "    print(f\"Label: {label}\")\n",
        "\n",
        "random_idx = random.choice(range(len(df)))\n",
        "visualize(random_idx)\n",
        "\n",
        "random_idx = random.choice(range(len(df)))\n",
        "visualize(random_idx)\n"
      ],
      "metadata": {
        "id": "eTB-ELzXXpVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-Test split\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'].values, random_state=42)\n",
        "\n",
        "# validation set\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.05, stratify=train_df['label'].values, random_state=42)\n",
        "\n",
        "print(f\"Total train examples: {len(train_df)}\")\n",
        "print(f\"Total val examples: {len(val_df)}\")\n",
        "print(f\"Total test examples: {len(test_df)}\""
      ],
      "metadata": {
        "id": "t3nO1-uBalKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data input pipeline\n",
        "text_preprocessor = keras_hub.models.BertTextClassifierPreprocessor.from_preset(\"bert_base_en_uncased\", sequence_lengh=128,\n",
        ")"
      ],
      "metadata": {
        "id": "4j0O8FJCalQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the preprocessor on a sample input\n",
        "idx = random.choice(range(len(train_df)))\n",
        "sample_input = train_df.iloc[idx]\n",
        "sample_text_1 = sample_input[\"text_1\"]\n",
        "sample_text_2 = sample_input[\"text_2\"]\n",
        "\n",
        "print(f\"Text 1: {sample_text_1}\")\n",
        "print(f\"Text 2: {sample_text_2}\")\n",
        "\n",
        "processed_text = text_preprocessor([sample_text_1, sample_text_2])\n",
        "print(processed_text)\n",
        "\n",
        "print(\"Keys            : \", list(processed_text.keys()))\n",
        "print(\"Shape Token Ids : \", processed_text['token_ids'].shape)\n",
        "print(\"Token Ids       : \", processed_text['token_ids'][0, :16])\n",
        "print(\"Shape Padding Masks     : \", processed_text['padding_mask'].shape)\n",
        "print(\"Padding Masks     : \", processed_text['padding_mask'][0, :16])\n",
        "print(\"Shape Segment Ids : \", processed_text['segment_ids'].shape)\n",
        "print(\"Segment Ids       : \", processed_text['segment_ids'][0, :16])"
      ],
      "metadata": {
        "id": "IZaq6M0ralTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tf.data.Dataset objects from dataframes\n",
        "def dataframe_to_dataset(dataframe):\n",
        "    columns = ['image_1_path', 'image_2_path', 'text_1', 'text_2', 'label_idx']\n",
        "    dataset = UnifiedPyDataset(dataframe, batch_size=32, workers=4)\n",
        "    return dataset\n",
        "\n",
        "# Preprocessing utiliteis\n",
        "bert_input_features = ['padding_mask', 'segment_ids', 'token_ids']\n",
        "def preprocess_text(text_1, text_2):\n",
        "    output = text_preprocessor([text_1, text_2])\n",
        "    return {feature: keras.ops.reshape(output[feature], [-1]) for feature in bert_input_features}"
      ],
      "metadata": {
        "id": "YY8jf_gFalWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnifiedPyDataset(PyDataset):\n",
        "    \"\"\"A Keras-compatible dataset that processes a DataFrame for TensorFlow, JAX, and PyTorch.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        df,\n",
        "        batch_size=32,\n",
        "        workers=4,\n",
        "        use_multiprocessing=False,\n",
        "        max_queue_size=10,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            df: pandas DataFrame with data\n",
        "            batch_size: Batch size for dataset\n",
        "            workers: Number of workers to use for parallel loading (Keras)\n",
        "            use_multiprocessing: Whether to use multiprocessing\n",
        "            max_queue_size: Maximum size of the data queue for parallel loading\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.dataframe = df\n",
        "        columns = [\"image_1_path\", \"image_2_path\", \"text_1\", \"text_2\"]\n",
        "\n",
        "        # image files\n",
        "        self.image_x_1 = self.dataframe[\"image_1_path\"]\n",
        "        self.image_x_2 = self.dataframe[\"image_1_path\"]\n",
        "        self.image_y = self.dataframe[\"label_idx\"]\n",
        "\n",
        "        # text files\n",
        "        self.text_x_1 = self.dataframe[\"text_1\"]\n",
        "        self.text_x_2 = self.dataframe[\"text_2\"]\n",
        "        self.text_y = self.dataframe[\"label_idx\"]\n",
        "\n",
        "        # general\n",
        "        self.batch_size = batch_size\n",
        "        self.workers = workers\n",
        "        self.use_multiprocessing = use_multiprocessing\n",
        "        self.max_queue_size = max_queue_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Fetches a batch of data from the dataset at the given index.\n",
        "        \"\"\"\n",
        "\n",
        "        # Return x, y for batch idx.\n",
        "        low = index * self.batch_size\n",
        "        # Cap upper bound at array length; the last batch may be smaller\n",
        "        # if the total number of items is not a multiple of batch size.\n",
        "\n",
        "        high_image_1 = min(low + self.batch_size, len(self.image_x_1))\n",
        "        high_image_2 = min(low + self.batch_size, len(self.image_x_2))\n",
        "\n",
        "        high_text_1 = min(low + self.batch_size, len(self.text_x_1))\n",
        "        high_text_2 = min(low + self.batch_size, len(self.text_x_1))\n",
        "\n",
        "        # images files\n",
        "        batch_image_x_1 = self.image_x_1[low:high_image_1]\n",
        "        batch_image_y_1 = self.image_y[low:high_image_1]\n",
        "\n",
        "        batch_image_x_2 = self.image_x_2[low:high_image_2]\n",
        "        batch_image_y_2 = self.image_y[low:high_image_2]\n",
        "\n",
        "        # text files\n",
        "        batch_text_x_1 = self.text_x_1[low:high_text_1]\n",
        "        batch_text_y_1 = self.text_y[low:high_text_1]\n",
        "\n",
        "        batch_text_x_2 = self.text_x_2[low:high_text_2]\n",
        "        batch_text_y_2 = self.text_y[low:high_text_2]\n",
        "\n",
        "        # image number 1 inputs\n",
        "        image_1 = [\n",
        "            resize(imread(file_name), (128, 128)) for file_name in batch_image_x_1\n",
        "        ]\n",
        "        image_1 = [\n",
        "            (  # exeperienced some shapes which were different from others.\n",
        "                np.array(Image.fromarray((img.astype(np.uint8))).convert(\"RGB\"))\n",
        "                if img.shape[2] == 4\n",
        "                else img\n",
        "            )\n",
        "            for img in image_1\n",
        "        ]\n",
        "        image_1 = np.array(image_1)\n",
        "\n",
        "        # Both text inputs to the model, return a dict for inputs to BertBackbone\n",
        "        text = {\n",
        "            key: np.array(\n",
        "                [\n",
        "                    d[key]\n",
        "                    for d in [\n",
        "                        preprocess_text(file_path1, file_path2)\n",
        "                        for file_path1, file_path2 in zip(\n",
        "                            batch_text_x_1, batch_text_x_2\n",
        "                        )\n",
        "                    ]\n",
        "                ]\n",
        "            )\n",
        "            for key in [\"padding_mask\", \"token_ids\", \"segment_ids\"]\n",
        "        }\n",
        "\n",
        "        # Image number 2 model inputs\n",
        "        image_2 = [\n",
        "            resize(imread(file_name), (128, 128)) for file_name in batch_image_x_2\n",
        "        ]\n",
        "        image_2 = [\n",
        "            (  # exeperienced some shapes which were different from others\n",
        "                np.array(Image.fromarray((img.astype(np.uint8))).convert(\"RGB\"))\n",
        "                if img.shape[2] == 4\n",
        "                else img\n",
        "            )\n",
        "            for img in image_2\n",
        "        ]\n",
        "        # Stack the list comprehension to an nd.array\n",
        "        image_2 = np.array(image_2)\n",
        "\n",
        "        return (\n",
        "            {\n",
        "                \"image_1\": image_1,\n",
        "                \"image_2\": image_2,\n",
        "                \"padding_mask\": text[\"padding_mask\"],\n",
        "                \"segment_ids\": text[\"segment_ids\"],\n",
        "                \"token_ids\": text[\"token_ids\"],\n",
        "            },\n",
        "            # Target lables\n",
        "            np.array(batch_image_y_1),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the number of batches in the dataset.\n",
        "        \"\"\"\n",
        "        return math.ceil(len(self.dataframe) / self.batch_size)"
      ],
      "metadata": {
        "id": "IiAd9zm2alZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train, validation and test datasets\n",
        "def prepare_dataset(dataframe):\n",
        "    ds = dataframe_to_dataset(dataframe)\n",
        "    return ds\n",
        "\n",
        "\n",
        "train_ds = prepare_dataset(train_df)\n",
        "validation_ds = prepare_dataset(val_df)\n",
        "test_ds = prepare_dataset(test_df)"
      ],
      "metadata": {
        "id": "zP01FOMnalcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Building\n",
        "# The model will take 2 images along with corresponding texts.\n",
        "# The images can be fed directly to the model while the text will have to be preprocessed\n",
        "# The model consist of Image encoder (ResNet50V2) and text encoder pretrained (BERT)\n",
        "\n",
        "def project_embeddings(embeddings, num_projection_layers, projection_dims, dropout_rate):\n",
        "    projected_embeddings = keras.layers.Dense(units=projection_dims)(embeddings)\n",
        "    for _ in range(num_projection_layers):\n",
        "        x = keras.ops.nn.gelu(projected_embeddings)\n",
        "        x = keras.layers.Dense(projection_dims)(x)\n",
        "        x = keras.layers.Dropout(dropout_rate)(x)\n",
        "        x = keras.layers.Add()([projected_embeddings, x])\n",
        "        projected_embeddings = keras.layers.LayerNormalization()(x)\n",
        "    return projected_embeddings"
      ],
      "metadata": {
        "id": "cEQYjAQslv1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visual encoder utilities\n",
        "def create_vision_encoder(\n",
        "        num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
        "        ):\n",
        "    # Load the pre-trained ResNet50V2 model to be used as the base encoder.\n",
        "    resnet_v2 = keras.applications.ResNet50V2(\n",
        "        include_top=False, weights=\"imagenet\", pooling=\"avg\"\n",
        "    )\n",
        "    # Set the trainability of the base encoder.\n",
        "    for layer in resnet_v2.layers:\n",
        "        layer.trainable = trainable\n",
        "\n",
        "    # Receive the images as inputs.\n",
        "    image_1 = keras.Input(shape=(128, 128, 3), name=\"image_1\")\n",
        "    image_2 = keras.Input(shape=(128, 128, 3), name=\"image_2\")\n",
        "\n",
        "    # Preprocess the input image.\n",
        "    preprocessed_1 = keras.applications.resnet_v2.preprocess_input(image_1)\n",
        "    preprocessed_2 = keras.applications.resnet_v2.preprocess_input(image_2)\n",
        "\n",
        "    # Generate the embeddings for the images using the resnet_v2 model\n",
        "    # concatenate them.\n",
        "    embeddings_1 = resnet_v2(preprocessed_1)\n",
        "    embeddings_2 = resnet_v2(preprocessed_2)\n",
        "    embeddings = keras.layers.Concatenate()([embeddings_1, embeddings_2])\n",
        "\n",
        "    # Project the embeddings produced by the model.\n",
        "    outputs = project_embeddings(\n",
        "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
        "    )\n",
        "    # Create the vision encoder model.\n",
        "    return keras.Model([image_1, image_2], outputs, name=\"vision_encoder\")"
      ],
      "metadata": {
        "id": "0ITiGEqdm-Ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_text_encoder(\n",
        "        num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
        "        ):\n",
        "    # Load the pre-trained BERT BackBone using KerasHub.\n",
        "    bert = keras_hub.models.BertBackbone.from_preset(\n",
        "        \"bert_base_en_uncased\", num_classes=3\n",
        "    )\n",
        "\n",
        "    # Set the trainability of the base encoder.\n",
        "    bert.trainable = trainable\n",
        "\n",
        "    # Receive the text as inputs.\n",
        "    bert_input_features = [\"padding_mask\", \"segment_ids\", \"token_ids\"]\n",
        "    inputs = {\n",
        "        feature: keras.Input(shape=(256,), dtype=\"int32\", name=feature)\n",
        "        for feature in bert_input_features\n",
        "    }\n",
        "\n",
        "    # Generate embeddings for the preprocessed text using the BERT model.\n",
        "    embeddings = bert(inputs)[\"pooled_output\"]\n",
        "\n",
        "    # Project the embeddings produced by the model.\n",
        "    outputs = project_embeddings(\n",
        "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
        "    )\n",
        "    # Create the text encoder model.\n",
        "    return keras.Model(inputs, outputs, name=\"text_encoder\")"
      ],
      "metadata": {
        "id": "VmZ2QRJyqjto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create multimodal model\n",
        "def create_multimodal_model(\n",
        "        num_projection_layers=1,\n",
        "        projection_dims=256,\n",
        "        dropout_rate=0.1,\n",
        "        vision_trainable=False,\n",
        "        text_trainable=False,\n",
        "        ):\n",
        "    # Receive the images as inputs.\n",
        "    image_1 = keras.Input(shape=(128, 128, 3), name=\"image_1\")\n",
        "    image_2 = keras.Input(shape=(128, 128, 3), name=\"image_2\")\n",
        "\n",
        "    # Receive the text as inputs.\n",
        "    bert_input_features = [\"padding_mask\", \"segment_ids\", \"token_ids\"]\n",
        "    text_inputs = {\n",
        "        feature: keras.Input(shape=(256,), dtype=\"int32\", name=feature)\n",
        "        for feature in bert_input_features\n",
        "    }\n",
        "    text_inputs = list(text_inputs.values())\n",
        "    # Create the encoders.\n",
        "    vision_encoder = create_vision_encoder(\n",
        "        num_projection_layers, projection_dims, dropout_rate, vision_trainable\n",
        "    )\n",
        "    text_encoder = create_text_encoder(\n",
        "        num_projection_layers, projection_dims, dropout_rate, text_trainable\n",
        "    )\n",
        "\n",
        "    # Fetch the embedding projections.\n",
        "    vision_projections = vision_encoder([image_1, image_2])\n",
        "    text_projections = text_encoder(text_inputs)\n",
        "\n",
        "    # Concatenate the projections and pass through the classification layer.\n",
        "    concatenated = keras.layers.Concatenate()([vision_projections, text_projections])\n",
        "    outputs = keras.layers.Dense(3, activation=\"softmax\")(concatenated)\n",
        "    return keras.Model([image_1, image_2, *text_inputs], outputs)\n",
        "\n",
        "\n",
        "multimodal_model = create_multimodal_model()\n",
        "keras.utils.plot_model(multimodal_model, show_shapes=True)"
      ],
      "metadata": {
        "id": "yWCAaRoLqj0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilte and train\n",
        "multimodal_model.compile(\n",
        "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history = multimodal_model.fit(train_ds, validation_data=validation_ds, epochs=1)"
      ],
      "metadata": {
        "id": "MmmFKn22qj37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "_, acc = multimodal_model.evaluate(test_ds)\n",
        "print(f\"Accuracy on the test set: {round(acc * 100, 2)}%.\")"
      ],
      "metadata": {
        "id": "2hoaEQMLqj8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Appendix"
      ],
      "metadata": {
        "id": "mu7GHkk6r75n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we can introduce cross attention to ensure the model focuses on part(s) of the image that relate to the corresponding textual input.\n",
        "\n",
        "# Embeddings.\n",
        "vision_projections = vision_encoder([image_1, image_2])\n",
        "text_projections = text_encoder(text_inputs)\n",
        "\n",
        "# Cross-attention (Luong-style).\n",
        "query_value_attention_seq = keras.layers.Attention(use_scale=True, dropout=0.2)(\n",
        "    [vision_projections, text_projections]\n",
        ")\n",
        "# Concatenate.\n",
        "concatenated = keras.layers.Concatenate()([vision_projections, text_projections])\n",
        "contextual = keras.layers.Concatenate()([concatenated, query_value_attention_seq])"
      ],
      "metadata": {
        "id": "TYJO6Gg9r-8r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}