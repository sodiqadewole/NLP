{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Multimodal entailment involves analysis of text, images, audio, and video data sources to determine if a piece of information contradict another or whether a given piece of information implies the other. This is applied is social media content moderation where platform operators audits and moderate content."
      ],
      "metadata": {
        "id": "uuCqdUs4Vsx3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vswQBJYZVl5Y"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "os.environ['KERAS_BACKEND'] = 'jax'\n",
        "\n",
        "import keras\n",
        "import keras_hub\n",
        "from keras.utils import PyDataset"
      ],
      "metadata": {
        "id": "B-gUuHXAWsh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a label map\n",
        "label_map = {\n",
        "    'contradiction': 0,\n",
        "    'entailment': 1,\n",
        "    'neutral': 2\n",
        "}"
      ],
      "metadata": {
        "id": "qVlyTNDoWso1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect dataset\n",
        "image_base_path = keras.utils.get_file(\n",
        "    \"tweet_images\",\n",
        "    \"https://github.com/sayakpaul/Multimodal-Entailment-Baseline/releases/download/v1.0.0/tweet_images.tar.gz\",\n",
        "    untar=True,\n",
        ")\n",
        "\n",
        "# Read dataset and apply preprocessing to the first 1k samples\n",
        "df = pd.read_csv(\n",
        "    \"https://github.com/sayakpaul/Multimodal-Entailment-Baseline/raw/main/csvs/tweets.csv\"\n",
        ").iloc[0:1000]\n",
        "\n",
        "df.sample(10)"
      ],
      "metadata": {
        "id": "Ot9MJD5JWsr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We formulate the entailment task as - given pairs of {text_1, image_1} and {text_2, image_2}, predict (contradict, entail, or neutral)\n",
        "\n",
        "images_1_paths = []\n",
        "images_2_paths = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    current_row = df.iloc[i]\n",
        "    id_1 = current_row[\"id_1\"]\n",
        "    id_2 = current_row[\"id_2\"]\n",
        "    extension_1 = current_row[\"image_1\"].split(\".\")[-1]\n",
        "    extension_2 = current_row[\"image_2\"].split(\".\")[-1]\n",
        "    image_1_path = os.path.join(image_base_path, f\"{id_1}.{extension_1}\")\n",
        "    image_2_path = os.path.join(image_base_path, f\"{id_2}.{extension_2}\")\n",
        "    images_1_paths.append(image_1_path)\n",
        "    images_2_paths.append(image_2_path)\n",
        "\n",
        "df[\"image_1_path\"] = images_1_paths\n",
        "df[\"image_2_path\"] = images_2_paths\n",
        "\n",
        "# Add label column\n",
        "df['label_idx'] = df['label'].apply(lambda x: label_map[x])"
      ],
      "metadata": {
        "id": "e1IV7DzcXpSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize sample dataset\n",
        "def visualize(idx):\n",
        "    current_row = df.iloc[idx]\n",
        "    image_1 = plt.imread(current_row[\"image_1_path\"])\n",
        "    image_2 = plt.imread(current_row[\"image_2_path\"])\n",
        "    text_1 = current_row[\"text_1\"]\n",
        "    text_2 = current_row[\"text_2\"]\n",
        "    label = current_row[\"label\"]\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image_1)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Image 1: {text_1}\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(image_2)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Image 2: {text_2}\")\n",
        "    plt.show()\n",
        "    print(f\"Label: {label}\")\n",
        "\n",
        "random_idx = random.choice(range(len(df)))\n",
        "visualize(random_idx)\n",
        "\n",
        "random_idx = random.choice(range(len(df)))\n",
        "visualize(random_idx)\n"
      ],
      "metadata": {
        "id": "eTB-ELzXXpVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-Test split\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'].values, random_state=42)\n",
        "\n",
        "# validation set\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.05, stratify=train_df['label'].values, random_state=42)\n",
        "\n",
        "print(f\"Total train examples: {len(train_df)}\")\n",
        "print(f\"Total val examples: {len(val_df)}\")\n",
        "print(f\"Total test examples: {len(test_df)}\""
      ],
      "metadata": {
        "id": "t3nO1-uBalKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data input pipeline\n",
        "text_preprocessor = keras_hub.models.BertTextClassifierPreprocessor.from_preset(\"bert_base_en_uncased\", sequence_lengh=128,\n",
        ")"
      ],
      "metadata": {
        "id": "4j0O8FJCalQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the preprocessor on a sample input\n",
        "idx = random.choice(range(len(train_df)))\n",
        "sample_input = train_df.iloc[idx]\n",
        "sample_text_1 = sample_input[\"text_1\"]\n",
        "sample_text_2 = sample_input[\"text_2\"]\n",
        "\n",
        "print(f\"Text 1: {sample_text_1}\")\n",
        "print(f\"Text 2: {sample_text_2}\")\n",
        "\n",
        "processed_text = text_preprocessor([sample_text_1, sample_text_2])\n",
        "print(processed_text)\n",
        "\n",
        "print(\"Keys            : \", list(processed_text.keys()))\n",
        "print(\"Shape Token Ids : \", processed_text['token_ids'].shape)\n",
        "print(\"Token Ids       : \", processed_text['token_ids'][0, :16])\n",
        "print(\"Shape Padding Masks     : \", processed_text['padding_mask'].shape)\n",
        "print(\"Padding Masks     : \", processed_text['padding_mask'][0, :16])\n",
        "print(\"Shape Segment Ids : \", processed_text['segment_ids'].shape)\n",
        "print(\"Segment Ids       : \", processed_text['segment_ids'][0, :16])"
      ],
      "metadata": {
        "id": "IZaq6M0ralTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tf.data.Dataset objects from dataframes\n",
        "def dataframe_to_dataset(dataframe):\n",
        "    columns = ['image_1_path', 'image_2_path', 'text_1', 'text_2', 'label_idx']\n",
        "    dataset = UnifiedPyDataset(dataframe, batch_size=32, workers=4)\n",
        "    return dataset\n",
        "\n",
        "# Preprocessing utiliteis\n",
        "bert_input_features = ['padding_mask', 'segment_ids', 'token_ids']\n",
        "def preprocess_text(text_1, text_2):\n",
        "    output = text_preprocessor([text_1, text_2])\n",
        "    return {feature: keras.ops.reshape(output[feature], [-1]) for feature in bert_input_features}"
      ],
      "metadata": {
        "id": "YY8jf_gFalWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnifiedPyDataset(PyDataset):\n",
        "    \"\"\"A Keras-compatible dataset that processes a DataFrame for TensorFlow, JAX, and PyTorch.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        df,\n",
        "        batch_size=32,\n",
        "        workers=4,\n",
        "        use_multiprocessing=False,\n",
        "        max_queue_size=10,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            df: pandas DataFrame with data\n",
        "            batch_size: Batch size for dataset\n",
        "            workers: Number of workers to use for parallel loading (Keras)\n",
        "            use_multiprocessing: Whether to use multiprocessing\n",
        "            max_queue_size: Maximum size of the data queue for parallel loading\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.dataframe = df\n",
        "        columns = [\"image_1_path\", \"image_2_path\", \"text_1\", \"text_2\"]\n",
        "\n",
        "        # image files\n",
        "        self.image_x_1 = self.dataframe[\"image_1_path\"]\n",
        "        self.image_x_2 = self.dataframe[\"image_1_path\"]\n",
        "        self.image_y = self.dataframe[\"label_idx\"]\n",
        "\n",
        "        # text files\n",
        "        self.text_x_1 = self.dataframe[\"text_1\"]\n",
        "        self.text_x_2 = self.dataframe[\"text_2\"]\n",
        "        self.text_y = self.dataframe[\"label_idx\"]\n",
        "\n",
        "        # general\n",
        "        self.batch_size = batch_size\n",
        "        self.workers = workers\n",
        "        self.use_multiprocessing = use_multiprocessing\n",
        "        self.max_queue_size = max_queue_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Fetches a batch of data from the dataset at the given index.\n",
        "        \"\"\"\n",
        "\n",
        "        # Return x, y for batch idx.\n",
        "        low = index * self.batch_size\n",
        "        # Cap upper bound at array length; the last batch may be smaller\n",
        "        # if the total number of items is not a multiple of batch size.\n",
        "\n",
        "        high_image_1 = min(low + self.batch_size, len(self.image_x_1))\n",
        "        high_image_2 = min(low + self.batch_size, len(self.image_x_2))\n",
        "\n",
        "        high_text_1 = min(low + self.batch_size, len(self.text_x_1))\n",
        "        high_text_2 = min(low + self.batch_size, len(self.text_x_1))\n",
        "\n",
        "        # images files\n",
        "        batch_image_x_1 = self.image_x_1[low:high_image_1]\n",
        "        batch_image_y_1 = self.image_y[low:high_image_1]\n",
        "\n",
        "        batch_image_x_2 = self.image_x_2[low:high_image_2]\n",
        "        batch_image_y_2 = self.image_y[low:high_image_2]\n",
        "\n",
        "        # text files\n",
        "        batch_text_x_1 = self.text_x_1[low:high_text_1]\n",
        "        batch_text_y_1 = self.text_y[low:high_text_1]\n",
        "\n",
        "        batch_text_x_2 = self.text_x_2[low:high_text_2]\n",
        "        batch_text_y_2 = self.text_y[low:high_text_2]\n",
        "\n",
        "        # image number 1 inputs\n",
        "        image_1 = [\n",
        "            resize(imread(file_name), (128, 128)) for file_name in batch_image_x_1\n",
        "        ]\n",
        "        image_1 = [\n",
        "            (  # exeperienced some shapes which were different from others.\n",
        "                np.array(Image.fromarray((img.astype(np.uint8))).convert(\"RGB\"))\n",
        "                if img.shape[2] == 4\n",
        "                else img\n",
        "            )\n",
        "            for img in image_1\n",
        "        ]\n",
        "        image_1 = np.array(image_1)\n",
        "\n",
        "        # Both text inputs to the model, return a dict for inputs to BertBackbone\n",
        "        text = {\n",
        "            key: np.array(\n",
        "                [\n",
        "                    d[key]\n",
        "                    for d in [\n",
        "                        preprocess_text(file_path1, file_path2)\n",
        "                        for file_path1, file_path2 in zip(\n",
        "                            batch_text_x_1, batch_text_x_2\n",
        "                        )\n",
        "                    ]\n",
        "                ]\n",
        "            )\n",
        "            for key in [\"padding_mask\", \"token_ids\", \"segment_ids\"]\n",
        "        }\n",
        "\n",
        "        # Image number 2 model inputs\n",
        "        image_2 = [\n",
        "            resize(imread(file_name), (128, 128)) for file_name in batch_image_x_2\n",
        "        ]\n",
        "        image_2 = [\n",
        "            (  # exeperienced some shapes which were different from others\n",
        "                np.array(Image.fromarray((img.astype(np.uint8))).convert(\"RGB\"))\n",
        "                if img.shape[2] == 4\n",
        "                else img\n",
        "            )\n",
        "            for img in image_2\n",
        "        ]\n",
        "        # Stack the list comprehension to an nd.array\n",
        "        image_2 = np.array(image_2)\n",
        "\n",
        "        return (\n",
        "            {\n",
        "                \"image_1\": image_1,\n",
        "                \"image_2\": image_2,\n",
        "                \"padding_mask\": text[\"padding_mask\"],\n",
        "                \"segment_ids\": text[\"segment_ids\"],\n",
        "                \"token_ids\": text[\"token_ids\"],\n",
        "            },\n",
        "            # Target lables\n",
        "            np.array(batch_image_y_1),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the number of batches in the dataset.\n",
        "        \"\"\"\n",
        "        return math.ceil(len(self.dataframe) / self.batch_size)"
      ],
      "metadata": {
        "id": "IiAd9zm2alZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train, validation and test datasets\n",
        "def prepare_dataset(dataframe):\n",
        "    ds = dataframe_to_dataset(dataframe)\n",
        "    return ds\n",
        "\n",
        "\n",
        "train_ds = prepare_dataset(train_df)\n",
        "validation_ds = prepare_dataset(val_df)\n",
        "test_ds = prepare_dataset(test_df)"
      ],
      "metadata": {
        "id": "zP01FOMnalcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Building\n",
        "# The model will take 2 images along with corresponding texts.\n",
        "# The images can be fed directly to the model while the text will have to be preprocessed\n",
        "# The model consist of Image encoder (ResNet50V2) and text encoder pretrained (BERT)\n",
        "\n",
        "def project_embeddings(embeddings, num_projection_layers, projection_dims, dropout_rate):\n",
        "    projected_embeddings = keras.layers.Dense(units=projection_dims)(embeddings)\n",
        "    for _ in range(num_projection_layers):\n",
        "        x = keras.ops.nn.gelu(projected_embeddings)\n",
        "        x = keras.layers.Dense(projection_dims)(x)\n",
        "        x = keras.layers.Dropout(dropout_rate)(x)\n",
        "        x = keras.layers.Add()([projected_embeddings, x])\n",
        "        projected_embeddings = keras.layers.LayerNormalization()(x)\n",
        "    return projected_embeddings"
      ],
      "metadata": {
        "id": "cEQYjAQslv1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visual encoder utilities\n",
        "def create_vision_encoder(\n",
        "        num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
        "        ):\n",
        "    # Load the pre-trained ResNet50V2 model to be used as the base encoder.\n",
        "    resnet_v2 = keras.applications.ResNet50V2(\n",
        "        include_top=False, weights=\"imagenet\", pooling=\"avg\"\n",
        "    )\n",
        "    # Set the trainability of the base encoder.\n",
        "    for layer in resnet_v2.layers:\n",
        "        layer.trainable = trainable\n",
        "\n",
        "    # Receive the images as inputs.\n",
        "    image_1 = keras.Input(shape=(128, 128, 3), name=\"image_1\")\n",
        "    image_2 = keras.Input(shape=(128, 128, 3), name=\"image_2\")\n",
        "\n",
        "    # Preprocess the input image.\n",
        "    preprocessed_1 = keras.applications.resnet_v2.preprocess_input(image_1)\n",
        "    preprocessed_2 = keras.applications.resnet_v2.preprocess_input(image_2)\n",
        "\n",
        "    # Generate the embeddings for the images using the resnet_v2 model\n",
        "    # concatenate them.\n",
        "    embeddings_1 = resnet_v2(preprocessed_1)\n",
        "    embeddings_2 = resnet_v2(preprocessed_2)\n",
        "    embeddings = keras.layers.Concatenate()([embeddings_1, embeddings_2])\n",
        "\n",
        "    # Project the embeddings produced by the model.\n",
        "    outputs = project_embeddings(\n",
        "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
        "    )\n",
        "    # Create the vision encoder model.\n",
        "    return keras.Model([image_1, image_2], outputs, name=\"vision_encoder\")"
      ],
      "metadata": {
        "id": "0ITiGEqdm-Ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_text_encoder(\n",
        "        num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
        "        ):\n",
        "    # Load the pre-trained BERT BackBone using KerasHub.\n",
        "    bert = keras_hub.models.BertBackbone.from_preset(\n",
        "        \"bert_base_en_uncased\", num_classes=3\n",
        "    )\n",
        "\n",
        "    # Set the trainability of the base encoder.\n",
        "    bert.trainable = trainable\n",
        "\n",
        "    # Receive the text as inputs.\n",
        "    bert_input_features = [\"padding_mask\", \"segment_ids\", \"token_ids\"]\n",
        "    inputs = {\n",
        "        feature: keras.Input(shape=(256,), dtype=\"int32\", name=feature)\n",
        "        for feature in bert_input_features\n",
        "    }\n",
        "\n",
        "    # Generate embeddings for the preprocessed text using the BERT model.\n",
        "    embeddings = bert(inputs)[\"pooled_output\"]\n",
        "\n",
        "    # Project the embeddings produced by the model.\n",
        "    outputs = project_embeddings(\n",
        "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
        "    )\n",
        "    # Create the text encoder model.\n",
        "    return keras.Model(inputs, outputs, name=\"text_encoder\")"
      ],
      "metadata": {
        "id": "VmZ2QRJyqjto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create multimodal model\n",
        "def create_multimodal_model(\n",
        "        num_projection_layers=1,\n",
        "        projection_dims=256,\n",
        "        dropout_rate=0.1,\n",
        "        vision_trainable=False,\n",
        "        text_trainable=False,\n",
        "        ):\n",
        "    # Receive the images as inputs.\n",
        "    image_1 = keras.Input(shape=(128, 128, 3), name=\"image_1\")\n",
        "    image_2 = keras.Input(shape=(128, 128, 3), name=\"image_2\")\n",
        "\n",
        "    # Receive the text as inputs.\n",
        "    bert_input_features = [\"padding_mask\", \"segment_ids\", \"token_ids\"]\n",
        "    text_inputs = {\n",
        "        feature: keras.Input(shape=(256,), dtype=\"int32\", name=feature)\n",
        "        for feature in bert_input_features\n",
        "    }\n",
        "    text_inputs = list(text_inputs.values())\n",
        "    # Create the encoders.\n",
        "    vision_encoder = create_vision_encoder(\n",
        "        num_projection_layers, projection_dims, dropout_rate, vision_trainable\n",
        "    )\n",
        "    text_encoder = create_text_encoder(\n",
        "        num_projection_layers, projection_dims, dropout_rate, text_trainable\n",
        "    )\n",
        "\n",
        "    # Fetch the embedding projections.\n",
        "    vision_projections = vision_encoder([image_1, image_2])\n",
        "    text_projections = text_encoder(text_inputs)\n",
        "\n",
        "    # Concatenate the projections and pass through the classification layer.\n",
        "    concatenated = keras.layers.Concatenate()([vision_projections, text_projections])\n",
        "    outputs = keras.layers.Dense(3, activation=\"softmax\")(concatenated)\n",
        "    return keras.Model([image_1, image_2, *text_inputs], outputs)\n",
        "\n",
        "\n",
        "multimodal_model = create_multimodal_model()\n",
        "keras.utils.plot_model(multimodal_model, show_shapes=True)"
      ],
      "metadata": {
        "id": "yWCAaRoLqj0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilte and train\n",
        "multimodal_model.compile(\n",
        "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history = multimodal_model.fit(train_ds, validation_data=validation_ds, epochs=1)"
      ],
      "metadata": {
        "id": "MmmFKn22qj37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "_, acc = multimodal_model.evaluate(test_ds)\n",
        "print(f\"Accuracy on the test set: {round(acc * 100, 2)}%.\")"
      ],
      "metadata": {
        "id": "2hoaEQMLqj8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Appendix"
      ],
      "metadata": {
        "id": "mu7GHkk6r75n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we can introduce cross attention to ensure the model focuses on part(s) of the image that relate to the corresponding textual input.\n",
        "\n",
        "# Embeddings.\n",
        "vision_projections = vision_encoder([image_1, image_2])\n",
        "text_projections = text_encoder(text_inputs)\n",
        "\n",
        "# Cross-attention (Luong-style).\n",
        "query_value_attention_seq = keras.layers.Attention(use_scale=True, dropout=0.2)(\n",
        "    [vision_projections, text_projections]\n",
        ")\n",
        "# Concatenate.\n",
        "concatenated = keras.layers.Concatenate()([vision_projections, text_projections])\n",
        "contextual = keras.layers.Concatenate()([concatenated, query_value_attention_seq])"
      ],
      "metadata": {
        "id": "TYJO6Gg9r-8r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}